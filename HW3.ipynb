{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyucheng113/hw3/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dBIMBMpa8LZ"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdshpulRajCG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optims\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import InterpolationMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwp7lRWlbX_s",
        "outputId": "e5e20fb4-a3ff-4ecd-df93-3c52048ff0fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 0 GPUs\n",
            "CUDA is available: False\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda')\n",
        "gpu_count = torch.cuda.device_count()\n",
        "\n",
        "print(\"Using\", gpu_count, \"GPUs\")\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-QHGGMMbBur"
      },
      "outputs": [],
      "source": [
        "# train loop\n",
        "def train_one_epoch(model, device, criterion, optimizer, train_data_loader):\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate (train_data_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(images)\n",
        "        loss = criterion(preds, labels.unsqueeze(1))\n",
        "\n",
        "        # Calculating Loss\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        # Calculating Metrics\n",
        "        predicts = (preds > 0.5).float()\n",
        "        predicts = predicts.view(-1)\n",
        "        predicts = predicts.detach().cpu().numpy()\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "        acc = accuracy_score(labels, predicts)\n",
        "\n",
        "        epoch_acc.append(acc)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Overall Epoch Results\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    epoch_acc = np.mean(epoch_acc) * 100\n",
        "\n",
        "    return epoch_loss, epoch_acc, total_time\n",
        "\n",
        "# validation loop\n",
        "def val_one_epoch(model, device, criterion, val_data_loader, best_acc, save):\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "            preds = model(images)\n",
        "\n",
        "            # Calculating Loss\n",
        "            loss = criterion(preds, labels.unsqueeze(1))\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "            # Calculating Metrics\n",
        "            predicts = (preds > 0.5).float()\n",
        "            predicts = predicts.view(-1)\n",
        "            predicts = predicts.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            acc = accuracy_score(labels, predicts)\n",
        "            epoch_acc.append(acc)\n",
        "\n",
        "    # Overall Epoch Results\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    epoch_acc = np.mean(epoch_acc) * 100\n",
        "\n",
        "    # Saving best model\n",
        "    if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        torch.save(model.state_dict(), f\"{save}.pth\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, total_time, best_acc\n",
        "\n",
        "# evaluate loop (test loop)\n",
        "def evaluate(model, device, model_path, test_loader):\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(\"Model weights loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: Failed to load model weights. Using randomly initialized weights instead.\")\n",
        "        print(e)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "\n",
        "            # Forward pass\n",
        "            preds = model(images)\n",
        "\n",
        "            # Calculating loss\n",
        "            loss = criterion(preds, labels.unsqueeze(1))\n",
        "            test_loss.append(loss.item())\n",
        "\n",
        "            # Calculating accuracy\n",
        "            predicts = (preds > 0.5).float()\n",
        "            predicts = predicts.view(-1)\n",
        "            predicts = predicts.detach().cpu().numpy()\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            acc = accuracy_score(labels, predicts)\n",
        "            test_acc.append(acc)\n",
        "\n",
        "    # Overall test results\n",
        "    avg_test_loss = np.mean(test_loss)\n",
        "    avg_test_acc = np.mean(test_acc) * 100\n",
        "\n",
        "    print(f\"Test Accuracy: {avg_test_acc:.2f}%\")\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "    return avg_test_loss, avg_test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Mz1-uybSmC"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWULx9svbXJD",
        "outputId": "0b7e04b9-ff3b-4a77-d46a-c5b201d07fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpL62qzsbkcj"
      },
      "outputs": [],
      "source": [
        "# define folder path for each set\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/chest_xray /train'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/chest_xray /test'\n",
        "val_path = '/content/drive/MyDrive/Colab Notebooks/chest_xray /val'\n",
        "\n",
        "# define transformation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "common_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), interpolation=InterpolationMode.BICUBIC),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# create datasets\n",
        "train_dataset = datasets.ImageFolder(train_path, transform=train_transform)\n",
        "test_dataset = datasets.ImageFolder(test_path, transform=common_transform)\n",
        "val_dataset = datasets.ImageFolder(val_path, transform=common_transform)\n",
        "\n",
        "# create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GJycZFHncviw",
        "outputId": "164e196e-f02b-4952-ad56-81c511f8ed98"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5e751b3a3622>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print the sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "train_size = len(train_dataset)\n",
        "test_size = len(test_dataset)\n",
        "val_size = len(val_dataset)\n",
        "\n",
        "# Print the sizes\n",
        "print(f\"Train dataset size: {train_size}\")\n",
        "print(f\"Test dataset size: {test_size}\")\n",
        "print(f\"Validation dataset size: {val_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxPrGZFPmvoO"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml-6AVUimusM",
        "outputId": "ff1111f0-742d-4385-c2b6-21069d05967a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=65536, out_features=64, bias=True)\n",
            "  (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (3): ReLU()\n",
            "  (4): Dropout(p=0.4, inplace=False)\n",
            "  (5): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (7): ReLU()\n",
            "  (8): Dropout(p=0.4, inplace=False)\n",
            "  (9): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU()\n",
            "  (12): Dropout(p=0.4, inplace=False)\n",
            "  (13): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (14): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256 * 256 * 1, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "\n",
        "    nn.Linear(64, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "\n",
        "    nn.Linear(64, 64),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "\n",
        "    nn.Linear(64, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdTAY4X1CXUu"
      },
      "source": [
        "# Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_EzCFAjCWp9"
      },
      "outputs": [],
      "source": [
        "# hyperparameter\n",
        "lr = 0.00008\n",
        "weight_decay = 0.001\n",
        "epochs = 20\n",
        "optimizer = optims.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3, mode='max')\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# save checkpoint\n",
        "save = 'model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxdqJu8wCae3",
        "outputId": "97b56b90-df67-487f-ac67-a476f458046b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - loss: 0.6025 - train_acc: 70.40% - val_loss: 0.6220 - val_acc: 68.75% - time: 89.73s\n",
            "Epoch 2/20 - loss: 0.4891 - train_acc: 83.65% - val_loss: 0.5874 - val_acc: 87.50% - time: 89.85s\n",
            "Epoch 3/20 - loss: 0.4068 - train_acc: 89.75% - val_loss: 0.4817 - val_acc: 93.75% - time: 90.44s\n",
            "Epoch 4/20 - loss: 0.3266 - train_acc: 93.55% - val_loss: 0.5952 - val_acc: 56.25% - time: 91.44s\n",
            "Epoch 5/20 - loss: 0.3019 - train_acc: 93.35% - val_loss: 0.5436 - val_acc: 62.50% - time: 90.26s\n",
            "Epoch 6/20 - loss: 0.2594 - train_acc: 94.50% - val_loss: 0.3495 - val_acc: 93.75% - time: 90.23s\n",
            "Epoch 7/20 - loss: 0.2265 - train_acc: 95.00% - val_loss: 0.4623 - val_acc: 81.25% - time: 91.05s\n",
            "Epoch 8/20 - loss: 0.2220 - train_acc: 94.80% - val_loss: 0.3678 - val_acc: 93.75% - time: 91.27s\n",
            "Epoch 9/20 - loss: 0.2058 - train_acc: 95.85% - val_loss: 0.3757 - val_acc: 87.50% - time: 91.82s\n",
            "Epoch 10/20 - loss: 0.2046 - train_acc: 95.45% - val_loss: 0.4050 - val_acc: 81.25% - time: 90.58s\n",
            "Epoch 11/20 - loss: 0.2043 - train_acc: 96.20% - val_loss: 0.3556 - val_acc: 93.75% - time: 90.38s\n",
            "Epoch 12/20 - loss: 0.2057 - train_acc: 95.35% - val_loss: 0.3607 - val_acc: 87.50% - time: 90.66s\n",
            "Epoch 13/20 - loss: 0.1900 - train_acc: 96.80% - val_loss: 0.3915 - val_acc: 87.50% - time: 90.68s\n",
            "Epoch 14/20 - loss: 0.2019 - train_acc: 96.10% - val_loss: 0.3669 - val_acc: 87.50% - time: 91.08s\n",
            "Epoch 15/20 - loss: 0.1997 - train_acc: 95.45% - val_loss: 0.3516 - val_acc: 87.50% - time: 90.33s\n",
            "Epoch 16/20 - loss: 0.1957 - train_acc: 95.90% - val_loss: 0.3652 - val_acc: 87.50% - time: 90.64s\n",
            "Epoch 17/20 - loss: 0.1881 - train_acc: 96.60% - val_loss: 0.3430 - val_acc: 87.50% - time: 90.28s\n",
            "Epoch 18/20 - loss: 0.1919 - train_acc: 96.15% - val_loss: 0.3640 - val_acc: 87.50% - time: 90.18s\n",
            "Epoch 19/20 - loss: 0.1995 - train_acc: 95.65% - val_loss: 0.3474 - val_acc: 87.50% - time: 91.70s\n",
            "Epoch 20/20 - loss: 0.1996 - train_acc: 95.90% - val_loss: 0.3541 - val_acc: 87.50% - time: 91.41s\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "best_acc = 0.0\n",
        "output_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc, train_time = train_one_epoch(model, device, criterion, optimizer, train_loader)\n",
        "    val_loss, val_acc, val_time, best_acc = val_one_epoch(model, device, criterion, val_loader, best_acc, save)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    total_time = train_time + val_time\n",
        "    output_str = f\"Epoch {epoch+1}/{epochs} - loss: {train_loss:.4f} - train_acc: {train_acc:.2f}% - val_loss: {val_loss:.4f} - val_acc: {val_acc:.2f}% - time: {total_time:.2f}s\"\n",
        "    output_list.append(output_str)\n",
        "    print(output_str)\n",
        "    lr_scheduler.step(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii620X77cmvS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGz-VUPmCgf4"
      },
      "source": [
        "# Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "an1QmoF0CjAV",
        "outputId": "cc0818c7-3ac9-4052-b6df-e6db3fa96058"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7d3846575f71>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# loss graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# loss graph\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# accuracy graph\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "# show\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5W5mX4KH0eg"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50oemy4aH2nW",
        "outputId": "f149cce1-1334-493a-c6e8-256fe77a498d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-54ee45e3a212>:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights loaded successfully.\n",
            "Test Accuracy: 83.97%\n",
            "Test Loss: 0.3848\n"
          ]
        }
      ],
      "source": [
        "model_path = 'model.pth'\n",
        "avg_test_loss, avg_test_acc = evaluate(model, device, model_path, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}